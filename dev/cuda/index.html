<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>CUDA · OMEinsum.jl</title><meta name="title" content="CUDA · OMEinsum.jl"/><meta property="og:title" content="CUDA · OMEinsum.jl"/><meta property="twitter:title" content="CUDA · OMEinsum.jl"/><meta name="description" content="Documentation for OMEinsum.jl."/><meta property="og:description" content="Documentation for OMEinsum.jl."/><meta property="twitter:description" content="Documentation for OMEinsum.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OMEinsum.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../background/">Background: Tensor Networks</a></li><li><a class="tocitem" href="../basic/">Basic usage</a></li><li><a class="tocitem" href="../contractionorder/">Contraction order optimization</a></li><li><a class="tocitem" href="../autodiff/">Automatic differentiation</a></li><li class="is-active"><a class="tocitem" href>CUDA</a><ul class="internal"><li><a class="tocitem" href="#Basic-CUDA-Usage"><span>Basic CUDA Usage</span></a></li><li><a class="tocitem" href="#GPU-Backends"><span>GPU Backends</span></a></li><li><a class="tocitem" href="#Using-the-cuTENSOR-Backend"><span>Using the cuTENSOR Backend</span></a></li><li><a class="tocitem" href="#Nested-Contractions-with-cuTENSOR"><span>Nested Contractions with cuTENSOR</span></a></li><li><a class="tocitem" href="#Troubleshooting"><span>Troubleshooting</span></a></li><li><a class="tocitem" href="#Video-Tutorial"><span>Video Tutorial</span></a></li></ul></li><li><a class="tocitem" href="../applications/">Applications</a></li><li><a class="tocitem" href="../docstrings/">Manual</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>CUDA</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>CUDA</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/under-Peter/OMEinsum.jl/blob/master/docs/src/cuda.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="CUDA-Acceleration"><a class="docs-heading-anchor" href="#CUDA-Acceleration">CUDA Acceleration</a><a id="CUDA-Acceleration-1"></a><a class="docs-heading-anchor-permalink" href="#CUDA-Acceleration" title="Permalink"></a></h1><p>OMEinsum supports GPU acceleration through CUDA.jl. By uploading your data to the GPU, you can significantly accelerate tensor contractions.</p><h2 id="Basic-CUDA-Usage"><a class="docs-heading-anchor" href="#Basic-CUDA-Usage">Basic CUDA Usage</a><a id="Basic-CUDA-Usage-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-CUDA-Usage" title="Permalink"></a></h2><pre><code class="language-julia hljs">julia&gt; using CUDA, OMEinsum

julia&gt; code = ein&quot;ij,jk,kl,li-&gt;&quot;  # the einsum notation
ij, jk, kl, li -&gt; 

julia&gt; A, B, C, D = rand(1000, 1000), rand(1000, 300), rand(300, 800), rand(800, 1000);

julia&gt; size_dict = OMEinsum.get_size_dict(getixsv(code), (A, B, C, D))  # get the size of the labels
Dict{Char, Int64} with 4 entries:
  &#39;j&#39; =&gt; 1000
  &#39;i&#39; =&gt; 1000
  &#39;k&#39; =&gt; 300
  &#39;l&#39; =&gt; 800

julia&gt; optcode = optimize_code(code, size_dict, TreeSA())  # optimize the contraction order
SlicedEinsum{Char, DynamicNestedEinsum{Char}}(Char[], kl, kl -&gt; 
├─ ki, li -&gt; kl
│  ├─ jk, ij -&gt; ki
│  │  ├─ jk
│  │  └─ ij
│  └─ li
└─ kl
)</code></pre><p>The contraction order is optimized. Now, let&#39;s benchmark the contraction on the CPU.</p><pre><code class="language-julia hljs">julia&gt; using BenchmarkTools

julia&gt; @btime optcode($A, $B, $C, $D)  # the contraction on CPU
  6.053 ms (308 allocations: 20.16 MiB)
0-dimensional Array{Float64, 0}:
1.4984046443610943e10</code></pre><p>The contraction on the CPU takes about 6 ms. Now, let&#39;s upload the data to the GPU and perform the contraction on the GPU.</p><pre><code class="language-julia hljs">julia&gt; cuA, cuB, cuC, cuD = CuArray(A), CuArray(B), CuArray(C), CuArray(D);

julia&gt; @btime CUDA.@sync optcode($cuA, $cuB, $cuC, $cuD)  # the contraction on GPU
  243.888 μs (763 allocations: 28.56 KiB)
0-dimensional CuArray{Float64, 0, CUDA.DeviceMemory}:
1.4984046443610939e10</code></pre><h2 id="GPU-Backends"><a class="docs-heading-anchor" href="#GPU-Backends">GPU Backends</a><a id="GPU-Backends-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Backends" title="Permalink"></a></h2><p>OMEinsum provides two backends for GPU tensor contractions:</p><table><tr><th style="text-align: right">Backend</th><th style="text-align: right">Library</th><th style="text-align: right">Best For</th></tr><tr><td style="text-align: right"><code>DefaultBackend()</code></td><td style="text-align: right">CUBLAS</td><td style="text-align: right">Matrix-like contractions (GEMM patterns)</td></tr><tr><td style="text-align: right"><code>CuTensorBackend()</code></td><td style="text-align: right">cuTENSOR</td><td style="text-align: right">General tensor contractions</td></tr></table><h3 id="DefaultBackend-(CUBLAS)"><a class="docs-heading-anchor" href="#DefaultBackend-(CUBLAS)">DefaultBackend (CUBLAS)</a><a id="DefaultBackend-(CUBLAS)-1"></a><a class="docs-heading-anchor-permalink" href="#DefaultBackend-(CUBLAS)" title="Permalink"></a></h3><p>The default backend uses NVIDIA&#39;s CUBLAS library. It works by:</p><ol><li>Analyzing the contraction pattern</li><li>Reshaping tensors to fit matrix multiplication (GEMM) format</li><li>Calling <code>CUBLAS.gemm_strided_batched!</code></li><li>Reshaping the result back</li></ol><p>This approach works well for contractions that naturally map to matrix multiplication, such as:</p><ul><li><code>ein&quot;ij,jk-&gt;ik&quot;</code> (matrix multiplication)</li><li><code>ein&quot;ijl,jkl-&gt;ikl&quot;</code> (batched matrix multiplication)</li></ul><h3 id="CuTensorBackend-(cuTENSOR)"><a class="docs-heading-anchor" href="#CuTensorBackend-(cuTENSOR)">CuTensorBackend (cuTENSOR)</a><a id="CuTensorBackend-(cuTENSOR)-1"></a><a class="docs-heading-anchor-permalink" href="#CuTensorBackend-(cuTENSOR)" title="Permalink"></a></h3><p>The cuTENSOR backend uses NVIDIA&#39;s cuTENSOR library, which provides <strong>native tensor contraction</strong> without the reshape/permute overhead. This is especially beneficial for:</p><ul><li><strong>Non-GEMM patterns</strong>: Contractions like <code>ein&quot;ijk,jkl-&gt;il&quot;</code> that don&#39;t naturally fit GEMM</li><li><strong>High-dimensional tensors</strong>: Avoids costly permutations</li><li><strong>Complex index patterns</strong>: Direct support for arbitrary contraction patterns</li></ul><h2 id="Using-the-cuTENSOR-Backend"><a class="docs-heading-anchor" href="#Using-the-cuTENSOR-Backend">Using the cuTENSOR Backend</a><a id="Using-the-cuTENSOR-Backend-1"></a><a class="docs-heading-anchor-permalink" href="#Using-the-cuTENSOR-Backend" title="Permalink"></a></h2><h3 id="Prerequisites"><a class="docs-heading-anchor" href="#Prerequisites">Prerequisites</a><a id="Prerequisites-1"></a><a class="docs-heading-anchor-permalink" href="#Prerequisites" title="Permalink"></a></h3><p>cuTENSOR requires:</p><ul><li>NVIDIA GPU with compute capability ≥ 6.0</li><li>CUDA.jl v5.0 or later</li><li>cuTENSOR.jl package (install with <code>Pkg.add(&quot;cuTENSOR&quot;)</code>)</li></ul><h3 id="Check-Availability"><a class="docs-heading-anchor" href="#Check-Availability">Check Availability</a><a id="Check-Availability-1"></a><a class="docs-heading-anchor-permalink" href="#Check-Availability" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CUDA
using cuTENSOR  # Must be loaded to enable the cuTENSOR backend
using OMEinsum

# Check if cuTENSOR is available
if cuTENSOR.has_cutensor()
    println(&quot;cuTENSOR is available!&quot;)
else
    println(&quot;cuTENSOR is not available&quot;)
end</code></pre><div class="admonition is-category-important" id="Loading-cuTENSOR-3974186f7738c8c2"><header class="admonition-header">Loading cuTENSOR<a class="admonition-anchor" href="#Loading-cuTENSOR-3974186f7738c8c2" title="Permalink"></a></header><div class="admonition-body"><p>cuTENSOR.jl is a <strong>separate package</strong>. You must:</p><ol><li>Install it: <code>Pkg.add(&quot;cuTENSOR&quot;)</code></li><li>Load it <strong>before</strong> or alongside OMEinsum: <code>using cuTENSOR</code></li></ol><p>This triggers OMEinsum&#39;s <code>CuTENSORExt</code> extension, which provides the cuTENSOR backend functionality.</p></div></div><h3 id="Enable-cuTENSOR-Backend"><a class="docs-heading-anchor" href="#Enable-cuTENSOR-Backend">Enable cuTENSOR Backend</a><a id="Enable-cuTENSOR-Backend-1"></a><a class="docs-heading-anchor-permalink" href="#Enable-cuTENSOR-Backend" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CUDA
using cuTENSOR  # ← Required! This loads the CuTENSORExt extension
using OMEinsum

# Set the backend globally
set_einsum_backend!(CuTensorBackend())

# Now all GPU einsum operations will use cuTENSOR
A = CUDA.rand(Float32, 100, 200, 300)
B = CUDA.rand(Float32, 200, 300, 400)
C = ein&quot;ijk,jkl-&gt;il&quot;(A, B)  # Uses cuTENSOR

# Reset to default backend
set_einsum_backend!(DefaultBackend())</code></pre><div class="admonition is-warning" id="Forgetting-to-load-cuTENSOR-2a49f67143dea8de"><header class="admonition-header">Forgetting to load cuTENSOR<a class="admonition-anchor" href="#Forgetting-to-load-cuTENSOR-2a49f67143dea8de" title="Permalink"></a></header><div class="admonition-body"><p>If you set <code>CuTensorBackend()</code> without loading cuTENSOR.jl, you&#39;ll see:</p><pre><code class="nohighlight hljs">┌ Warning: CuTensorBackend: cuTENSOR.jl not loaded - run `using cuTENSOR` first. Will fall back to CUBLAS.</code></pre><p>The computation will still work (using CUBLAS), but you won&#39;t get the cuTENSOR optimization.</p></div></div><h3 id="Supported-Data-Types"><a class="docs-heading-anchor" href="#Supported-Data-Types">Supported Data Types</a><a id="Supported-Data-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Data-Types" title="Permalink"></a></h3><p>The cuTENSOR backend supports BLAS-compatible types:</p><ul><li><code>Float16</code>, <code>Float32</code>, <code>Float64</code></li><li><code>ComplexF16</code>, <code>ComplexF32</code>, <code>ComplexF64</code></li></ul><p>For other types (e.g., <code>Double64</code>, custom number types), the backend automatically falls back to the loop-based implementation.</p><h3 id="Example:-Performance-Comparison"><a class="docs-heading-anchor" href="#Example:-Performance-Comparison">Example: Performance Comparison</a><a id="Example:-Performance-Comparison-1"></a><a class="docs-heading-anchor-permalink" href="#Example:-Performance-Comparison" title="Permalink"></a></h3><pre><code class="language-julia hljs">using CUDA
using cuTENSOR  # Required for cuTENSOR backend
using OMEinsum, BenchmarkTools

# Create test tensors (non-GEMM pattern)
A = CUDA.rand(Float32, 64, 64, 64)
B = CUDA.rand(Float32, 64, 64, 64)

# Benchmark with DefaultBackend (CUBLAS)
set_einsum_backend!(DefaultBackend())
@btime CUDA.@sync ein&quot;ijk,jkl-&gt;il&quot;($A, $B)
# Requires: permute → reshape → GEMM → reshape

# Benchmark with CuTensorBackend
set_einsum_backend!(CuTensorBackend())
@btime CUDA.@sync ein&quot;ijk,jkl-&gt;il&quot;($A, $B)
# Direct tensor contraction - no intermediate steps!</code></pre><h3 id="When-to-Use-cuTENSOR"><a class="docs-heading-anchor" href="#When-to-Use-cuTENSOR">When to Use cuTENSOR</a><a id="When-to-Use-cuTENSOR-1"></a><a class="docs-heading-anchor-permalink" href="#When-to-Use-cuTENSOR" title="Permalink"></a></h3><table><tr><th style="text-align: right">Use Case</th><th style="text-align: right">Recommended Backend</th></tr><tr><td style="text-align: right">Matrix multiplication <code>ij,jk-&gt;ik</code></td><td style="text-align: right">Either (similar performance)</td></tr><tr><td style="text-align: right">Batched matmul <code>ijl,jkl-&gt;ikl</code></td><td style="text-align: right">Either (similar performance)</td></tr><tr><td style="text-align: right">Tensor contraction <code>ijk,jkl-&gt;il</code></td><td style="text-align: right"><strong>CuTensorBackend</strong></td></tr><tr><td style="text-align: right">High-dimensional <code>ijkl,klmn-&gt;ijmn</code></td><td style="text-align: right"><strong>CuTensorBackend</strong></td></tr><tr><td style="text-align: right">MPS/PEPS contractions</td><td style="text-align: right"><strong>CuTensorBackend</strong></td></tr><tr><td style="text-align: right">Non-BLAS types (Double64, etc.)</td><td style="text-align: right">DefaultBackend</td></tr></table><h3 id="Best-Practices"><a class="docs-heading-anchor" href="#Best-Practices">Best Practices</a><a id="Best-Practices-1"></a><a class="docs-heading-anchor-permalink" href="#Best-Practices" title="Permalink"></a></h3><ol><li><p><strong>Use cuTENSOR for tensor networks</strong>: If you&#39;re doing MPS, PEPS, or general tensor network contractions, cuTENSOR typically provides better performance.</p></li><li><p><strong>Profile your workload</strong>: The relative performance depends on tensor sizes and contraction patterns. Use <code>BenchmarkTools</code> to measure.</p></li><li><p><strong>Keep data on GPU</strong>: Minimize CPU-GPU transfers by keeping intermediate results on the GPU.</p></li><li><p><strong>Batch operations</strong>: When contracting many small tensors, consider batching them together.</p></li></ol><pre><code class="language-julia hljs"># Good: Keep data on GPU throughout
cuA, cuB, cuC = CuArray.((A, B, C))
result = ein&quot;(ij,jk),kl-&gt;il&quot;(cuA, cuB, cuC)

# Avoid: Repeated transfers
result = ein&quot;ij,jk-&gt;ik&quot;(CuArray(A), CuArray(B))  # Transfer every call</code></pre><h2 id="Nested-Contractions-with-cuTENSOR"><a class="docs-heading-anchor" href="#Nested-Contractions-with-cuTENSOR">Nested Contractions with cuTENSOR</a><a id="Nested-Contractions-with-cuTENSOR-1"></a><a class="docs-heading-anchor-permalink" href="#Nested-Contractions-with-cuTENSOR" title="Permalink"></a></h2><p>The cuTENSOR backend works seamlessly with optimized contraction orders:</p><pre><code class="language-julia hljs">using CUDA, cuTENSOR, OMEinsum

set_einsum_backend!(CuTensorBackend())

# Define a tensor network
code = ein&quot;ij,jk,kl,lm-&gt;im&quot;

# Optimize contraction order
size_dict = Dict(&#39;i&#39;=&gt;100, &#39;j&#39;=&gt;100, &#39;k&#39;=&gt;100, &#39;l&#39;=&gt;100, &#39;m&#39;=&gt;100)
optcode = optimize_code(code, size_dict, TreeSA())

# Execute with cuTENSOR (each pairwise contraction uses cuTENSOR)
A, B, C, D = [CUDA.rand(Float32, 100, 100) for _ in 1:4]
result = optcode(A, B, C, D)</code></pre><h2 id="Troubleshooting"><a class="docs-heading-anchor" href="#Troubleshooting">Troubleshooting</a><a id="Troubleshooting-1"></a><a class="docs-heading-anchor-permalink" href="#Troubleshooting" title="Permalink"></a></h2><h3 id="cuTENSOR-not-detected"><a class="docs-heading-anchor" href="#cuTENSOR-not-detected">cuTENSOR not detected</a><a id="cuTENSOR-not-detected-1"></a><a class="docs-heading-anchor-permalink" href="#cuTENSOR-not-detected" title="Permalink"></a></h3><p>If <code>has_cutensor()</code> returns <code>false</code>:</p><ol><li><p><strong>Check CUDA.jl version</strong>: Ensure you have CUDA.jl v5.0+</p><pre><code class="language-julia hljs">using Pkg
Pkg.status(&quot;CUDA&quot;)</code></pre></li><li><p><strong>Check GPU compatibility</strong>: cuTENSOR requires compute capability ≥ 6.0</p><pre><code class="language-julia hljs">using CUDA
CUDA.capability(CUDA.device())</code></pre></li><li><p><strong>Reinstall CUDA artifacts</strong>:</p><pre><code class="language-julia hljs">using CUDA
CUDA.versioninfo()  # Check if cuTENSOR is listed</code></pre></li></ol><h3 id="Performance-not-as-expected"><a class="docs-heading-anchor" href="#Performance-not-as-expected">Performance not as expected</a><a id="Performance-not-as-expected-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-not-as-expected" title="Permalink"></a></h3><ol><li><strong>Ensure synchronization</strong>: Use <code>CUDA.@sync</code> when benchmarking</li><li><strong>Check tensor sizes</strong>: cuTENSOR has more overhead for very small tensors</li><li><strong>Verify backend is active</strong>: Check <code>get_einsum_backend()</code></li></ol><h2 id="Video-Tutorial"><a class="docs-heading-anchor" href="#Video-Tutorial">Video Tutorial</a><a id="Video-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#Video-Tutorial" title="Permalink"></a></h2><p>To learn more about using GPU and autodiff, please check out the following asciinema video. <a href="https://asciinema.org/a/wE4CtIzWUC3R0GkVV28rVBRFb"><img src="https://asciinema.org/a/wE4CtIzWUC3R0GkVV28rVBRFb.svg" alt="asciicast"/></a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../autodiff/">« Automatic differentiation</a><a class="docs-footer-nextpage" href="../applications/">Applications »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 26 December 2025 09:22">Friday 26 December 2025</span>. Using Julia version 1.12.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
