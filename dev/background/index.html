<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Background: Tensor Networks · OMEinsum.jl</title><meta name="title" content="Background: Tensor Networks · OMEinsum.jl"/><meta property="og:title" content="Background: Tensor Networks · OMEinsum.jl"/><meta property="twitter:title" content="Background: Tensor Networks · OMEinsum.jl"/><meta name="description" content="Documentation for OMEinsum.jl."/><meta property="og:description" content="Documentation for OMEinsum.jl."/><meta property="twitter:description" content="Documentation for OMEinsum.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">OMEinsum.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Background: Tensor Networks</a><ul class="internal"><li><a class="tocitem" href="#Tensors-and-Tensor-Networks"><span>Tensors and Tensor Networks</span></a></li><li><a class="tocitem" href="#Einsum-notation"><span>Einsum notation</span></a></li></ul></li><li><a class="tocitem" href="../basic/">Basic usage</a></li><li><a class="tocitem" href="../contractionorder/">Contraction order optimization</a></li><li><a class="tocitem" href="../autodiff/">Automatic differentiation</a></li><li><a class="tocitem" href="../cuda/">CUDA</a></li><li><a class="tocitem" href="../applications/">Applications</a></li><li><a class="tocitem" href="../docstrings/">Manual</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Background: Tensor Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Background: Tensor Networks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/under-Peter/OMEinsum.jl/blob/master/docs/src/background.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Background-Knowledge"><a class="docs-heading-anchor" href="#Background-Knowledge">Background Knowledge</a><a id="Background-Knowledge-1"></a><a class="docs-heading-anchor-permalink" href="#Background-Knowledge" title="Permalink"></a></h1><h2 id="Tensors-and-Tensor-Networks"><a class="docs-heading-anchor" href="#Tensors-and-Tensor-Networks">Tensors and Tensor Networks</a><a id="Tensors-and-Tensor-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Tensors-and-Tensor-Networks" title="Permalink"></a></h2><p>Tensor networks serve as a fundamental tool for modeling and analyzing correlated systems. This section reviews the fundamental concepts of tensor networks.</p><p>A tensor is a mathematical object that generalizes scalars, vectors, and matrices. It can have multiple dimensions and is used to represent data in various mathematical and physical contexts. It is formally defined as follows:</p><p><em>Definition</em> (Tensor): A tensor <span>$T$</span> associated to a set of discrete variables <span>$V$</span> is defined as a function that maps each possible instantiation of the variables in its scope <span>$\mathcal{D}_V = \prod_{v\in V} \mathcal{D}_{v}$</span> to an element in the set <span>$\mathcal{E}$</span>, given by</p><p class="math-container">\[T_{V}: \prod_{v \in V} \mathcal{D}_{v} \rightarrow \mathcal{E}.\]</p><p>Within the context of probabilistic modeling, the elements in <span>$\mathcal{E}$</span> are non-negative real numbers, while in other scenarios, they can be of generic types. The diagrammatic representation of a tensor is given by a node with the variables <span>$V$</span> as labels on its edges, as shown below:</p><img src="../assets/tensors.svg" width=500 style="margin-left:auto; margin-right:auto; display:block"/><p><em>Definition</em> (Tensor Network): A tensor network is a mathematical framework for defining multilinear maps, which can be represented by a triple <span>$\mathcal{N} = (\Lambda, \mathcal{T}, V_0)$</span>, where:</p><ul><li><span>$\Lambda$</span> is the set of variables present in the network <span>$\mathcal{N}$</span>.</li><li><span>$\mathcal{T} = \{ T_{V_k} \}_{k=1}^{K}$</span> is the set of input tensors, where each tensor <span>$T_{V_k}$</span> is associated with the labels <span>$V_k$</span>.</li><li><span>$V_0$</span> specifies the labels of the output tensor.</li></ul><p>Specifically, each tensor <span>$T_{V_k} \in \mathcal{T}$</span> is labeled by a set of variables <span>$V_k \subseteq \Lambda$</span>, where the cardinality <span>$|V_k|$</span> equals the rank of <span>$T_{V_k}$</span>. The multilinear map, or the <strong>contraction</strong>, applied to this triple is defined as</p><p class="math-container">\[T_{V_0} = \texttt{contract}(\Lambda, \mathcal{T}, V_0) \overset{\mathrm{def}}{=} \sum_{m \in \mathcal{D}_{\Lambda\setminus V_0}} \prod_{T_V \in \mathcal{T}} T_{V|M=m},\]</p><p>where <span>$M = \Lambda \setminus V_0$</span>. <span>$T_{V|M=m}$</span> denotes a slicing of the tensor <span>$T_{V}$</span> with the variables <span>$M$</span> fixed to the values <span>$m$</span>. The summation runs over all possible configurations of the variables in <span>$M$</span>.</p><p>For instance, matrix multiplication can be described as the contraction of a tensor network given by</p><p class="math-container">\[(AB)_{\{i, k\}} = \texttt{contract}\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right),\]</p><p>where matrices <span>$A$</span> and <span>$B$</span> are input tensors containing the variable sets <span>$\{i, j\}, \{j, k\}$</span>, respectively, which are subsets of <span>$\Lambda = \{i, j, k\}$</span>. The output tensor is comprised of variables <span>$\{i, k\}$</span> and the summation runs over variables <span>$\Lambda \setminus \{i, k\} = \{j\}$</span>. The contraction corresponds to</p><p class="math-container">\[(A B)_{\{i, k\}} = \sum_j A_{\{i,j\}}B_{\{j, k\}}.\]</p><p>Diagrammatically, a tensor network can be represented as an <em>open hypergraph</em>, where each tensor is mapped to a vertex and each variable is mapped to a hyperedge. Two vertices are connected by the same hyperedge if and only if they share a common variable. The diagrammatic representation of the matrix multiplication is given as follows: </p><img src="../assets/matmul.png" width=500 style="margin-left:auto; margin-right:auto; display:block"/><p>Here, we use different colors to denote different hyperedges. Hyperedges for <span>$i$</span> and <span>$k$</span> are left open to denote variables of the output tensor. A slightly more complex example of this is the star contraction:</p><p class="math-container">\[\texttt{contract}(\{i,j,k,l\}, \{A_{\{i, l\}}, B_{\{j, l\}}, C_{\{k, l\}}\}, \{i,j,k\}) \\
= \sum_{l}A_{\{i,l\}} B_{\{j,l\}} C_{\{k,l\}}.\]</p><p>Note that the variable <span>$l$</span> is shared by all three tensors, making regular edges, which by definition connect two nodes, insufficient for its representation. This motivates the need for hyperedges, which can connect a single variable to any number of nodes. The hypergraph representation is given as:</p><img src="../assets/starcontract.png" width=500 style="margin-left:auto; margin-right:auto; display:block"/><h2 id="Einsum-notation"><a class="docs-heading-anchor" href="#Einsum-notation">Einsum notation</a><a id="Einsum-notation-1"></a><a class="docs-heading-anchor-permalink" href="#Einsum-notation" title="Permalink"></a></h2><p>The einsum notation is a compact way to specify tensor contractions with a string. In this notation, an index (subscripts) is represented by a char, and the tensors are represented by the indices. The input tensors and the output tensor are separated by an arrow <code>-&gt;</code> and input tensors are separated by comma <code>,</code>. For example, the matrix multiplication <span>$\left(\{i,j,k\}, \{A_{\{i, j\}}, B_{\{j, k\}}\}, \{i, k\}\right)$</span> can be concisely written as <code>&quot;ij,jk-&gt;ik&quot;</code>. A general contraction can be defined with pseudocode as follows:</p><pre><code class="nohighlight hljs">Let A, B, C, ... be input tensors, O be the output tensor
for indices in domain_of_unique_indices(einsum_notation)
    O[indices in O] += A[indices in A] * B[indices in B] * ...
end</code></pre><h4 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h4><table><tr><th style="text-align: right">einsum notation</th><th style="text-align: right">meaning</th></tr><tr><td style="text-align: right"><code>ij,jk-&gt;ik</code></td><td style="text-align: right">matrix matrix multiplication</td></tr><tr><td style="text-align: right"><code>ijl,jkl-&gt;ikl</code></td><td style="text-align: right">batched - matrix matrix multiplication</td></tr><tr><td style="text-align: right"><code>ij,j-&gt;i</code></td><td style="text-align: right">matrix vector multiplication</td></tr><tr><td style="text-align: right"><code>ij,ik,il-&gt;jkl</code></td><td style="text-align: right">star contraction</td></tr><tr><td style="text-align: right"><code>ii-&gt;</code></td><td style="text-align: right">trace</td></tr><tr><td style="text-align: right"><code>ij-&gt;i</code></td><td style="text-align: right">sum</td></tr><tr><td style="text-align: right"><code>ii-&gt;i</code></td><td style="text-align: right">take the diagonal part of a matrix</td></tr><tr><td style="text-align: right"><code>ijkl-&gt;ilkj</code></td><td style="text-align: right">permute the dimensions of a tensor</td></tr><tr><td style="text-align: right"><code>i-&gt;ii</code></td><td style="text-align: right">construct a diagonal matrix</td></tr><tr><td style="text-align: right"><code>-&gt;ii</code></td><td style="text-align: right">broadcast a scalar to the diagonal part of a matrix</td></tr><tr><td style="text-align: right"><code>ij,ij-&gt;ij</code></td><td style="text-align: right">element wise product</td></tr><tr><td style="text-align: right"><code>ij,kl-&gt;ijkl</code></td><td style="text-align: right">outer product</td></tr></table><p>Please <a href="../basic/#Einsum-examples">Einsum examples</a> for code examples.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../basic/">Basic usage »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 26 December 2025 09:22">Friday 26 December 2025</span>. Using Julia version 1.12.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
