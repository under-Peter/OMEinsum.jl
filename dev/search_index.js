var documenterSearchIndex = {"docs":
[{"location":"cuda/#CUDA-Acceleration","page":"CUDA","title":"CUDA Acceleration","text":"","category":"section"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"By uploading your data to the GPU, you can accelerate the computation of your model.","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"julia> using CUDA, OMEinsum\n\njulia> code = ein\"ij,jk,kl,li->\"  # the einsum notation\nij, jk, kl, li -> \n\njulia> A, B, C, D = rand(1000, 1000), rand(1000, 300), rand(300, 800), rand(800, 1000);\n\njulia> size_dict = OMEinsum.get_size_dict(getixsv(code), (A, B, C, D))  # get the size of the labels\nDict{Char, Int64} with 4 entries:\n  'j' => 1000\n  'i' => 1000\n  'k' => 300\n  'l' => 800\n\njulia> optcode = optimize_code(code, size_dict, TreeSA())  # optimize the contraction order\nSlicedEinsum{Char, DynamicNestedEinsum{Char}}(Char[], kl, kl -> \n├─ ki, li -> kl\n│  ├─ jk, ij -> ki\n│  │  ├─ jk\n│  │  └─ ij\n│  └─ li\n└─ kl\n)","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"The contraction order is optimized. Now, let's benchmark the contraction on the CPU.","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"julia> using BenchmarkTools\n\njulia> @btime optcode($A, $B, $C, $D)  # the contraction on CPU\n  6.053 ms (308 allocations: 20.16 MiB)\n0-dimensional Array{Float64, 0}:\n1.4984046443610943e10","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"The contraction on the CPU takes about 6 ms. Now, let's upload the data to the GPU and perform the contraction on the GPU.","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"julia> @btime CUDA.@sync optcode($cuA, $cuB, $cuC, $cuD)  # the contraction on GPU\n  243.888 μs (763 allocations: 28.56 KiB)\n0-dimensional CuArray{Float64, 0, CUDA.DeviceMemory}:\n1.4984046443610939e10","category":"page"},{"location":"cuda/","page":"CUDA","title":"CUDA","text":"To learn more about using GPU and autodiff, please check out the following asciinema video. (Image: asciicast)","category":"page"},{"location":"contractionorder/#Contraction-order-optimization","page":"Contraction order optimization","title":"Contraction order optimization","text":"","category":"section"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"The @ein_str string literal does not optimize the contraction order for more than two input tensors.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"using OMEinsum\n\ncode = ein\"ij,jk,kl,li->\"","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"The return value is a StaticEinCode object that does not contain a contraction order. The time and space complexity can be obtained by calling the contraction_complexity function.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"size_dict = uniformsize(code, 10)  # size of the labels are set to 10\n\ncontraction_complexity(code, size_dict)  # time and space complexity","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"The return values are log2 values of the number of iterations, number of elements of the largest tensor and the number of elementwise read-write operations.","category":"page"},{"location":"contractionorder/#Optimizing-the-contraction-order","page":"Contraction order optimization","title":"Optimizing the contraction order","text":"","category":"section"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"To optimize the contraction order, we can use the optimize_code function.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"optcode = optimize_code(code, size_dict, TreeSA())","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"The output value is a binary contraction tree with type SlicedEinsum or NestedEinsum. The TreeSA is a local search algorithm that optimizes the contraction order. More algorithms can be found in the OMEinsumContractionOrders and the performance tips of GenericTensorNetworks.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"After optimizing the contraction order, the time and readwrite complexities are significantly reduced.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"contraction_complexity(optcode, size_dict)","category":"page"},{"location":"contractionorder/#Using-optein-string-literal","page":"Contraction order optimization","title":"Using optein string literal","text":"","category":"section"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"For convenience, the optimized contraction can be directly contructed by using the @optein_str string literal.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"optein\"ij,jk,kl,li->\"  # optimized contraction, without knowing the size of the tensors","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"The drawback of using @optein_str is that the contraction order is optimized without knowing the size of the tensors. Only the tensor ranks are used to optimize the contraction order.","category":"page"},{"location":"contractionorder/#Manual-optimization","page":"Contraction order optimization","title":"Manual optimization","text":"","category":"section"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"One can also manually specify the contraction order by using the @ein_str string literal.","category":"page"},{"location":"contractionorder/","page":"Contraction order optimization","title":"Contraction order optimization","text":"ein\"((ij,jk),kl),li->ik\"  # manually optimized contraction","category":"page"},{"location":"applications/#Application","page":"Applications","title":"Application","text":"","category":"section"},{"location":"applications/#List-of-packages-using-OMEinsum","page":"Applications","title":"List of packages using OMEinsum","text":"","category":"section"},{"location":"applications/","page":"Applications","title":"Applications","text":"GenericTensorNetworks, solving combinational optimization problems by generic tensor networks.\nTensorInference, probabilistic inference using contraction of tensor networks\nYaoToEinsum, the tensor network simulation backend for quantum circuits.\nTensorNetworkAD2, using differential programming tensor networks to solve quantum many-body problems.\nTensorQEC, tensor networks for quantum error correction.","category":"page"},{"location":"applications/#Example:-Solving-a-3-coloring-problem-on-the-Petersen-graph","page":"Applications","title":"Example: Solving a 3-coloring problem on the Petersen graph","text":"","category":"section"},{"location":"applications/","page":"Applications","title":"Applications","text":"Let us focus on graphs with vertices with three edges each. A question one might ask is: How many different ways are there to colour the edges of the graph with three different colours such that no vertex has a duplicate colour on its edges?","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"The counting problem can be transformed into a contraction of rank-3 tensors representing the edges. Consider the tensor s defined as","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"using OMEinsum\ns = map(x->Int(length(unique(x.I)) == 3), CartesianIndices((3,3,3)))","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"Then we can simply contract s tensors to get the number of 3 colourings satisfying the above condition! E.g. for two vertices, we get 6 distinct colourings:","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"ein\"ijk,ijk->\"(s,s)[]","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"Using that method, it's easy to find that e.g. the peterson graph allows no 3 colouring, since","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"code = ein\"afl,bhn,cjf,dlh,enj,ago,big,cki,dmk,eom->\"\nafl, bhn, cjf, dlh, enj, ago, big, cki, dmk, eom \ncode(fill(s, 10)...)[]","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"The peterson graph consists of 10 vertices and 15 edges and looks like a pentagram embedded in a pentagon as depicted here:","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"(Image: )","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"OMEinsum does not optimie the contraction order by default, so the above contraction can be time consuming. To speed up the contraction, we can use optimize_code to optimize the contraction order:","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"optcode = optimize_code(code, uniformsize(code, 3), TreeSA())\ncontraction_complexity(optcode, uniformsize(optcode, 3))\noptcode(fill(s, 10)...)[]","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"We can see the time complexity of the optimized code is much smaller than the original one. To know more about the contraction order optimization, please check the Julia package OMEinsumContractionOrders.jl.","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"Confronted with the above result, we can ask whether the peterson graph allows a relaxed variation of 3 colouring, having one vertex that might accept duplicate colours. The answer to that can be found using the gradient w.r.t a vertex:","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"using Zygote: gradient\ngradient(x->optcode(x,s,s,s,s,s,s,s,s,s)[], s)[1] |> sum","category":"page"},{"location":"applications/","page":"Applications","title":"Applications","text":"This tells us that even if we allow duplicates on one vertex, there are no 3-colourings for the peterson graph.","category":"page"},{"location":"basic/#Basic-Usage","page":"Basic usage","title":"Basic Usage","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"In the following example, we demonstrate the einsum notation for basic tensor operations.","category":"page"},{"location":"basic/#Einsum-notation","page":"Basic usage","title":"Einsum notation","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"To specify the operation, the user can either use the @ein_str-string literal or the EinCode object. For example, both the following code snippets define the matrix multiplication operation:","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"using OMEinsum\ncode1 = ein\"ij,jk -> ik\"  # the string literal\nixs = [[1, 2], [2, 3]]  # the input indices\niy = [1, 3]  # the output indices\ncode2 = EinCode(ixs, iy)  # the EinCode object (equivalent to the string literal)","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"The @ein_str macro can be used to define the einsum notation directly in the function call.","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"A, B = randn(2, 3), randn(3, 4);\ncode1(A, B)  # matrix multiplication\nsize_dict = OMEinsum.get_size_dict(getixsv(code1), (A, B))  # get the size of the labels\neinsum(code1, (A, B), size_dict)  # lower-level function\neinsum!(code1, (A, B), zeros(2, 4), true, false, size_dict)  # the in-place operation\n@ein C[i,k] := A[i,j] * B[j,k]  # all-in-one macro","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"Here, we show that the @ein macro combines the einsum notation defintion and the operation in a single line, which is more convenient for simple operations. Separating the einsum notation and the operation (the first approach) can be useful for reusing the einsum notation for multiple input tensors. Lower level functions, einsum and einsum!, can be used for more control over the operation.","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"For more than two input tensors, the @ein_str macro does not optimize the contraction order. In such cases, the user can use the @optein_str string literal to optimize the contraction order or specify the contraction order manually.","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"tensors = [randn(100, 100) for _ in 1:4];\noptein\"ij,jk,kl,lm->im\"(tensors...)  # optimized contraction (without knowing the size)\nein\"(ij,jk),(kl,lm)->im\"(tensors...)  # manually specified contraction","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"Sometimes, manually optimizing the contraction order can be beneficial. Please check Contraction order optimization for more details.","category":"page"},{"location":"basic/#Einsum-examples","page":"Basic usage","title":"Einsum examples","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"We first define the tensors and then demonstrate the einsum notation for various tensor operations.","category":"page"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"using OMEinsum\ns = fill(1)  # scalar\nw, v = [1, 2], [4, 5];  # vectors\nA, B = [1 2; 3 4], [5 6; 7 8]; # matrices\nT1, T2 = reshape(1:8, 2, 2, 2), reshape(9:16, 2, 2, 2); # 3D tensor","category":"page"},{"location":"basic/#Unary-examples","page":"Basic usage","title":"Unary examples","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"ein\"i->\"(w)  # sum of the elements of a vector.\nein\"ij->i\"(A)  # sum of the rows of a matrix.\nein\"ii->\"(A)  # sum of the diagonal elements of a matrix, i.e., the trace.\nein\"ij->\"(A)  # sum of the elements of a matrix.\nein\"i->ii\"(w)  # create a diagonal matrix.\nein\"i->ij\"(w; size_info=Dict('j'=>2))  # repeat a vector to form a matrix.\nein\"ijk->ikj\"(T1)  # permute the dimensions of a tensor.","category":"page"},{"location":"basic/#Binary-examples","page":"Basic usage","title":"Binary examples","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"ein\"ij, jk -> ik\"(A, B)  # matrix multiplication.\nein\"ijb,jkb->ikb\"(T1, T2)  # batch matrix multiplication.\nein\"ij,ij->ij\"(A, B)  # element-wise multiplication.\nein\"ij,ij->\"(A, B)  # sum of the element-wise multiplication.\nein\"ij,->ij\"(A, s)  # element-wise multiplication by a scalar.","category":"page"},{"location":"basic/#Nary-examples","page":"Basic usage","title":"Nary examples","text":"","category":"section"},{"location":"basic/","page":"Basic usage","title":"Basic usage","text":"optein\"ai,aj,ak->ijk\"(A, A, B)  # star contraction.\noptein\"ia,ajb,bkc,cld,dm->ijklm\"(A, T1, T2, T1, A)  # tensor train contraction.","category":"page"},{"location":"background/#Background-Knowledge","page":"Background: Tensor Networks","title":"Background Knowledge","text":"","category":"section"},{"location":"background/#Tensors-and-Tensor-Networks","page":"Background: Tensor Networks","title":"Tensors and Tensor Networks","text":"","category":"section"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Tensor networks serve as a fundamental tool for modeling and analyzing correlated systems. This section reviews the fundamental concepts of tensor networks.","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"A tensor is a mathematical object that generalizes scalars, vectors, and matrices. It can have multiple dimensions and is used to represent data in various mathematical and physical contexts. It is formally defined as follows:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Definition (Tensor): A tensor T associated to a set of discrete variables V is defined as a function that maps each possible instantiation of the variables in its scope mathcalD_V = prod_vin V mathcalD_v to an element in the set mathcalE, given by","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"T_V prod_v in V mathcalD_v rightarrow mathcalE","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Within the context of probabilistic modeling, the elements in mathcalE are non-negative real numbers, while in other scenarios, they can be of generic types. The diagrammatic representation of a tensor is given by a node with the variables V as labels on its edges, as shown below:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"<img src=\"../assets/tensors.svg\" width=500 style=\"margin-left:auto; margin-right:auto; display:block\"/>","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Definition (Tensor Network): A tensor network is a mathematical framework for defining multilinear maps, which can be represented by a triple mathcalN = (Lambda mathcalT V_0), where:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Lambda is the set of variables present in the network mathcalN.\nmathcalT =  T_V_k _k=1^K is the set of input tensors, where each tensor T_V_k is associated with the labels V_k.\nV_0 specifies the labels of the output tensor.","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Specifically, each tensor T_V_k in mathcalT is labeled by a set of variables V_k subseteq Lambda, where the cardinality V_k equals the rank of T_V_k. The multilinear map, or the contraction, applied to this triple is defined as","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"T_V_0 = textttcontract(Lambda mathcalT V_0) oversetmathrmdef= sum_m in mathcalD_Lambdasetminus V_0 prod_T_V in mathcalT T_VM=m","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"where M = Lambda setminus V_0. T_VM=m denotes a slicing of the tensor T_V with the variables M fixed to the values m. The summation runs over all possible configurations of the variables in M.","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"For instance, matrix multiplication can be described as the contraction of a tensor network given by","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"(AB)_i k = textttcontractleft(ijk A_i j B_j k i kright)","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"where matrices A and B are input tensors containing the variable sets i j j k, respectively, which are subsets of Lambda = i j k. The output tensor is comprised of variables i k and the summation runs over variables Lambda setminus i k = j. The contraction corresponds to","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"(A B)_i k = sum_j A_ijB_j k","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Diagrammatically, a tensor network can be represented as an open hypergraph, where each tensor is mapped to a vertex and each variable is mapped to a hyperedge. Two vertices are connected by the same hyperedge if and only if they share a common variable. The diagrammatic representation of the matrix multiplication is given as follows: ","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"<img src=\"../assets/matmul.png\" width=500 style=\"margin-left:auto; margin-right:auto; display:block\"/>","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Here, we use different colors to denote different hyperedges. Hyperedges for i and k are left open to denote variables of the output tensor. A slightly more complex example of this is the star contraction:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"textttcontract(ijkl A_i l B_j l C_k l ijk) \n= sum_lA_il B_jl C_kl","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Note that the variable l is shared by all three tensors, making regular edges, which by definition connect two nodes, insufficient for its representation. This motivates the need for hyperedges, which can connect a single variable to any number of nodes. The hypergraph representation is given as:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"<img src=\"../assets/starcontract.png\" width=500 style=\"margin-left:auto; margin-right:auto; display:block\"/>","category":"page"},{"location":"background/#Einsum-notation","page":"Background: Tensor Networks","title":"Einsum notation","text":"","category":"section"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"The einsum notation is a compact way to specify tensor contractions with a string. In this notation, an index (subscripts) is represented by a char, and the tensors are represented by the indices. The input tensors and the output tensor are separated by an arrow -> and input tensors are separated by comma ,. For example, the matrix multiplication left(ijk A_i j B_j k i kright) can be concisely written as \"ij,jk->ik\". A general contraction can be defined with pseudocode as follows:","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Let A, B, C, ... be input tensors, O be the output tensor\nfor indices in domain_of_unique_indices(einsum_notation)\n    O[indices in O] += A[indices in A] * B[indices in B] * ...\nend","category":"page"},{"location":"background/#Examples","page":"Background: Tensor Networks","title":"Examples","text":"","category":"section"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"einsum notation meaning\nij,jk->ik matrix matrix multiplication\nijl,jkl->ikl batched - matrix matrix multiplication\nij,j->i matrix vector multiplication\nij,ik,il->jkl star contraction\nii-> trace\nij->i sum\nii->i take the diagonal part of a matrix\nijkl->ilkj permute the dimensions of a tensor\ni->ii construct a diagonal matrix\n->ii broadcast a scalar to the diagonal part of a matrix\nij,ij->ij element wise product\nij,kl->ijkl outer product","category":"page"},{"location":"background/","page":"Background: Tensor Networks","title":"Background: Tensor Networks","text":"Please Einsum examples for code examples.","category":"page"},{"location":"#OMEinsum.jl","page":"Home","title":"OMEinsum.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides","category":"page"},{"location":"","page":"Home","title":"Home","text":"The einsum notation, which is similar to the einsum function in numpy, although some details are different.\nHighly optimized algorithms to optimize the contraction of tensors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The source code is available at OMEinsum.jl.","category":"page"},{"location":"#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can find a set up guide in the README. To get started, open a Julia REPL and type the following code.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using OMEinsum\ncode = ein\"ij,jk,kl,lm->im\" # define the einsum operation\noptcode = optimize_code(code, uniformsize(code, 100), TreeSA())  # optimize the contraction order\noptcode(randn(100, 100), randn(100, 100), randn(100, 100), randn(100, 100))  # compute the result","category":"page"},{"location":"autodiff/#Automatic-differentiation","page":"Automatic differentiation","title":"Automatic differentiation","text":"","category":"section"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"There are two ways to compute the gradient of an einsum expression. The first one is to use the OMEinsum package, which is a custom implementation of the reverse-mode automatic differentiation. The second one is to use the Zygote package, which is a source-to-source automatic differentiation tool.","category":"page"},{"location":"autodiff/#Built-in-automatic-differentiation","page":"Automatic differentiation","title":"Built-in automatic differentiation","text":"","category":"section"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"The OMEinsum package provides a built-in function cost_and_gradient to compute the cost and the gradient of an einsum expression.","category":"page"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"using OMEinsum  # the 1st way\nA, B, C = randn(2, 3), randn(3, 4), randn(4, 2);\ny, g = cost_and_gradient(ein\"(ij, jk), ki->\", (A, B, C))","category":"page"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"This built-in automatic differentiation is designed for tensor contractions and is more efficient than the general-purpose automatic differentiation tools.","category":"page"},{"location":"autodiff/#Using-Zygote","page":"Automatic differentiation","title":"Using Zygote","text":"","category":"section"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"The backward rule for the basic einsum operation is ported to the ChainRulesCore, which is used by the Zygote package. Zygote is a source-to-source automatic differentiation tool that can be used to compute the gradient of an einsum expression. It is more general and can be used for any Julia code.","category":"page"},{"location":"autodiff/","page":"Automatic differentiation","title":"Automatic differentiation","text":"using Zygote  # the 2nd way\nZygote.gradient((A, B, C)->ein\"(ij, jk), ki->\"(A, B, C)[], A, B, C)","category":"page"},{"location":"docstrings/#OMEinsum.DynamicEinCode","page":"Manual","title":"OMEinsum.DynamicEinCode","text":"DynamicEinCode{LT}\nDynamicEinCode(ixs, iy)\n\nWrapper to eincode-specification that creates a callable object to evaluate the eincode ixs -> iy where ixs are the index-labels of the input-tensors and iy are the index-labels of the output.\n\nexample\n\njulia> a, b = rand(2,2), rand(2,2);\n\njulia> OMEinsum.DynamicEinCode((('i','j'),('j','k')),('i','k'))(a, b) ≈ a * b\ntrue\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.DynamicNestedEinsum","page":"Manual","title":"OMEinsum.DynamicNestedEinsum","text":"DynamicNestedEinsum{LT} <: NestedEinsum{LT}\nDynamicNestedEinsum(args, eins)\nDynamicNestedEinsum{LT}(tensorindex::Int)\n\nEinsum with contraction order, where the type parameter LT is the label type. It has two constructors. One takes a tensorindex as input, which represents the leaf node in a contraction tree. The other takes an iterable of type DynamicNestedEinsum, args, as the siblings, and eins to specify the contraction operation.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.EinArray","page":"Manual","title":"OMEinsum.EinArray","text":"EinArray{T, N, TT, LX, LY, ICT, OCT} <: AbstractArray{T, N}\n\nA struct to hold the intermediate result of an einsum where all index-labels of both input and output are expanded to a rank-N-array whose values are lazily calculated. Indices are arranged as inner indices (or reduced dimensions) first and then outer indices.\n\nType parameters are\n\n* `T`: element type,\n* `N`: array dimension,\n* `TT`: type of \"tuple of input arrays\",\n* `LX`: type of \"tuple of input indexers\",\n* `LX`: type of output indexer,\n* `ICT`: typeof inner CartesianIndices,\n* `OCT`: typeof outer CartesianIndices,\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.EinCode","page":"Manual","title":"OMEinsum.EinCode","text":"EinCode <: AbstractEinsum\nEinCode(ixs, iy)\n\nAbstract type for sum-product contraction code. The constructor returns a DynamicEinCode instance.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.EinIndexer","page":"Manual","title":"OMEinsum.EinIndexer","text":"EinIndexer{locs,N}\n\nA structure for indexing EinArrays. locs is the index positions (among all indices). In the constructor, size is the size of target tensor,\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.EinIndexer-Union{Tuple{NTuple{N, Int64}}, Tuple{locs}, Tuple{N}} where {N, locs}","page":"Manual","title":"OMEinsum.EinIndexer","text":"EinIndexer{locs}(size::Tuple)\n\nConstructor for EinIndexer for an object of size size where locs are the locations of relevant indices in a larger tuple.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.IndexGroup","page":"Manual","title":"OMEinsum.IndexGroup","text":"IndexGroup\n\nLeaf in a contractiontree, contains the indices and the number of the tensor it describes, e.g. in \"ij,jk -> ik\", indices \"ik\" belong to tensor 1, so would be described by IndexGroup(['i','k'], 1).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.NestedEinsum","page":"Manual","title":"OMEinsum.NestedEinsum","text":"NestedEinsum{LT} <: AbstractEinsum\n\nThe abstract type for contraction trees. It has two subtypes, DynamicNestedEinsum and StaticNestedEinsum.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.NestedEinsumConstructor","page":"Manual","title":"OMEinsum.NestedEinsumConstructor","text":"NestedEinsumConstructor\n\ndescribes a (potentially) nested einsum. Important fields:\n\nargs, vector of all inputs, either IndexGroup objects corresponding to tensors or NestedEinsumConstructor\niy, indices of output\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.SlicedEinsum","page":"Manual","title":"OMEinsum.SlicedEinsum","text":"SlicedEinsum{LT, Ein} <: AbstractEinsum\n\nA tensor network with slicing. LT is the label type and Ein is the tensor network.\n\nFields\n\nslicing::Vector{LT}: A vector of labels to slice.\neins::Ein: The tensor network.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.StaticEinCode","page":"Manual","title":"OMEinsum.StaticEinCode","text":"StaticEinCode{LT, ixs, iy}\n\nThe static version of DynamicEinCode that matches the contraction rule at compile time. It is the default return type of @ein_str macro. LT is the label type.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsum.StaticNestedEinsum","page":"Manual","title":"OMEinsum.StaticNestedEinsum","text":"StaticNestedEinsum{LT,args,eins} <: NestedEinsum{LT}\nStaticNestedEinsum(args, eins)\nStaticNestedEinsum{LT}(tensorindex::Int)\n\nEinsum with contraction order, where the type parameter LT is the label type, args is a tuple of StaticNestedEinsum, eins is a StaticEinCode and leaf node is defined by setting eins to an integer. It has two constructors. One takes a tensorindex as input, which represents the leaf node in a contraction tree. The other takes an iterable of type DynamicNestedEinsum, args, as the siblings, and eins to specify the contraction operation.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#Base.getindex-Union{Tuple{T}, Tuple{EinArray{T}, Any}} where T","page":"Manual","title":"Base.getindex","text":"getindex(A::EinArray, inds...)\n\nreturn the lazily calculated entry of A at index inds.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.allow_loops-Tuple{Bool}","page":"Manual","title":"OMEinsum.allow_loops","text":"allow_loops(flag::Bool)\n\nSetting this to false will cause OMEinsum to log an error if it falls back to loop_einsum evaluation, instead of calling specialised kernels. The default is true.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.allunique-Tuple{Any}","page":"Manual","title":"OMEinsum.allunique","text":"allunique(ix::Tuple)\n\nreturn true if all elements of ix appear only once in ix.\n\nexample\n\njulia> using OMEinsum: allunique\n\njulia> allunique((1,2,3,4))\ntrue\n\njulia> allunique((1,2,3,1))\nfalse\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.analyze_binary-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, Vector{T}, Dict{T, Int64}}} where T","page":"Manual","title":"OMEinsum.analyze_binary","text":"Get the expected labels.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.asarray-Tuple{Any}","page":"Manual","title":"OMEinsum.asarray","text":"asarray(x[, parent::AbstractArray]) -> AbstactArray\n\nReturn a 0-dimensional array with item x, otherwise, do nothing. If a parent is supplied, it will try to match the parent array type.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.back_propagate-Union{Tuple{T}, Tuple{Any, SlicedEinsum, OMEinsum.CacheTree{T}, AbstractArray{T}, Dict}} where T","page":"Manual","title":"OMEinsum.back_propagate","text":"back_propagate(f, code, cache, ȳ, size_dict)\n\nBack propagate the message ȳ through the cached tree cache and return a tree storing the intermediate messages. The message can be gradients et al.\n\nArguments\n\nf: The back-propagation rule. The signature is f(eins, xs, y, size_dict, dy) -> dxs, where\neins: The contraction code at the current node.\nxs: The input tensors at the current node.\ny: The output tensor at the current node.\nsize_dict: The size dictionary, which maps the label to the size of the corresponding dimension.\ndy: The message on the output tensor (y) to back-propagate through the current node.\ndxs: The message on the input tensors (xs) as the result of back-propagation.\ncode: The contraction code, which can be a NestedEinsum or a SlicedEinsum.\ncache: The cached intermediate results, which can be generated by cached_einsum.\nȳ: The message to back-propagate.\nsize_dict: The size dictionary, which maps the label to the size of the corresponding dimension.\n\nReturns\n\nCacheTree: The tree storing the intermediate messages.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.cached_einsum-Tuple{SlicedEinsum, Any, Any}","page":"Manual","title":"OMEinsum.cached_einsum","text":"cached_einsum(code, xs, size_dict)\n\nCompute the einsum contraction and cache the intermediate contraction results.\n\nArguments\n\ncode: The contraction code, which can be a NestedEinsum or a SlicedEinsum.\nxs: The input tensors.\nsize_dict: The size dictionary, which maps the label to the size of the corresponding dimension.\n\nReturns\n\nCacheTree: The cached tree storing the intermediate results.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.cost_and_gradient","page":"Manual","title":"OMEinsum.cost_and_gradient","text":"cost_and_gradient(code, xs, ȳ)\n\nCompute the cost and the gradients w.r.t the input tensors xs.\n\nArguments\n\ncode: The contraction code, which can be a NestedEinsum or a SlicedEinsum.\nxs: The input tensors.\nȳ: The message to back-propagate. Default is 1.\n\nReturns\n\ncost: The cost of the contraction.\ngrads: The gradients w.r.t the input tensors.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#OMEinsum.einarray-Union{Tuple{TT}, Tuple{NI}, Tuple{iy}, Tuple{ixs}, Tuple{Val{ixs}, Val{iy}, TT, Any}} where {ixs, iy, NI, TT<:NTuple{NI, AbstractArray}}","page":"Manual","title":"OMEinsum.einarray","text":"einarray(::Val{ixs}, Val{iy}, xs, size_dict) -> EinArray\n\nConstructor of EinArray from an EinCode, a tuple of tensors xs and a size_dict that assigns each index-label a size. The returned EinArray holds an intermediate result of the einsum specified by the EinCode with indices corresponding to all unique labels in the einsum. Reduction over the (lazily calculated) dimensions that correspond to labels not present in the output lead to the result of the einsum.\n\nexample\n\njulia> using OMEinsum: get_size_dict\n\njulia> a, b = rand(2,2), rand(2,2);\n\njulia> sd = get_size_dict((('i','j'),('j','k')), (a, b));\n\njulia> ea = OMEinsum.einarray(Val((('i','j'),('j','k'))),Val(('i','k')), (a,b), sd);\n\njulia> dropdims(sum(ea, dims=1), dims=1) ≈ a * b\ntrue\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.einsum","page":"Manual","title":"OMEinsum.einsum","text":"einsum(code::EinCode, xs, size_dict)\n\nReturn the tensor that results from contracting the tensors xs according to the contraction code code.\n\nArguments\n\ncode: The einsum notation, which can be an instance of EinCode, NestedEinsum, or SlicedEinsum.\nxs - the input tensors\nsize_dict - a dictionary that maps index-labels to their sizes\n\nExamples\n\njulia> a, b = rand(2,2), rand(2,2);\n\njulia> einsum(EinCode((('i','j'),('j','k')),('i','k')), (a, b)) ≈ a * b\ntrue\n\njulia> einsum(EinCode((('i','j'),('j','k')),('k','i')), (a, b)) ≈ permutedims(a * b, (2,1))\ntrue\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#OMEinsum.einsum!","page":"Manual","title":"OMEinsum.einsum!","text":"einsum!(code::EinCode, xs, y, sx, sy, size_dict)\n\nInplace version of einsum. The result is stored in y.\n\nArguments\n\ncode: The einsum notation, which can be an instance of EinCode, NestedEinsum, or SlicedEinsum.\nxs: The input tensors.\ny: The output tensor.\nsx: Scale x by sx.\nsy: Scale y by sy.\nsize_dict: A dictionary that maps index-labels to their sizes.\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#OMEinsum.einsum_grad-NTuple{6, Any}","page":"Manual","title":"OMEinsum.einsum_grad","text":"einsum_grad(ixs, xs, iy, size_dict, cdy, i)\n\nreturn the gradient of the result of evaluating the EinCode w.r.t the ith tensor in xs. cdy is the result of applying the EinCode to the xs.\n\nexample\n\njulia> using OMEinsum: einsum_grad, get_size_dict\n\njulia> a, b = rand(2,2), rand(2,2);\n\njulia> c = einsum(EinCode((('i','j'),('j','k')), ('i','k')), (a,b));\n\njulia> sd = get_size_dict((('i','j'),('j','k')), (a,b));\n\njulia> einsum_grad((('i','j'),('j','k')), (a,b), ('i','k'), sd, c, 1) ≈ c * transpose(b)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.filliys!-Tuple{Any}","page":"Manual","title":"OMEinsum.filliys!","text":"filliys!(neinsum::NestedEinsumConstructor)\n\ngoes through all NestedEinsumConstructor objects in the tree and saves the correct iy in them.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.get_size_dict!-Union{Tuple{LT}, Tuple{Any, Any, Dict{LT}}} where LT","page":"Manual","title":"OMEinsum.get_size_dict!","text":"get_size_dict!(ixs, xs, size_info)\n\nreturn a dictionary that is used to get the size of an index-label in the einsum-specification with input-indices ixs and tensors xs after consistency within ixs and between ixs and xs has been verified.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.getixsv-Union{Tuple{StaticEinCode{LT}}, Tuple{LT}} where LT","page":"Manual","title":"OMEinsum.getixsv","text":"getixsv(code)\n\nGet labels of input tensors for EinCode, NestedEinsum and some other einsum like objects. Returns a vector of vectors.\n\njulia> getixsv(ein\"(ij,jk),k->i\")\n3-element Vector{Vector{Char}}:\n ['i', 'j']\n ['j', 'k']\n ['k']\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.getiyv-Union{Tuple{StaticEinCode{LT}}, Tuple{LT}} where LT","page":"Manual","title":"OMEinsum.getiyv","text":"getiy(code)\n\nGet labels of the output tensor for EinCode, NestedEinsum and some other einsum like objects. Returns a vector.\n\njulia> getiyv(ein\"(ij,jk),k->i\")\n1-element Vector{Char}:\n 'i': ASCII/Unicode U+0069 (category Ll: Letter, lowercase)\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.indices_and_locs-Tuple{Any, Any}","page":"Manual","title":"OMEinsum.indices_and_locs","text":"indices_and_locs(ixs,iy)\n\ngiven the index-labels of input and output of an einsum, return (in the same order):\n\na tuple of the distinct index-labels of the output iy\na tuple of the distinct index-labels in ixs of the input not appearing in the output iy\na tuple of tuples of locations of an index-label in the ixs in a list of all index-labels\na tuple of locations of index-labels in iy in a list of all index-labels\n\nwhere the list of all index-labels is simply the first  and the second output catenated and the second output catenated.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.loop_einsum!-Union{Tuple{T}, Tuple{L}, Tuple{N}, Tuple{Any, Any, NTuple{N, AbstractArray}, AbstractArray{T, L}, Any, Any, Any}} where {N, L, T}","page":"Manual","title":"OMEinsum.loop_einsum!","text":"loop_einsum!(ixs, iy, xs, y, sx, sy, size_dict)\n\ninplace-version of loop_einsum, saving the result in a preallocated tensor of correct size y.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.loop_einsum-Union{Tuple{N}, Tuple{EinCode, NTuple{N, AbstractArray}, Any}} where N","page":"Manual","title":"OMEinsum.loop_einsum","text":"loop_einsum(::EinCode, xs, size_dict)\n\nevaluates the eincode specified by EinCode and the tensors xs by looping over all possible indices and calculating the contributions ot the result. Scales exponentially in the number of distinct index-labels.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.map_prod-Union{Tuple{N}, Tuple{Tuple, Any, NTuple{N, Any}}} where N","page":"Manual","title":"OMEinsum.map_prod","text":"map_prod(xs, ind, indexers)\n\ncalculate the value of an EinArray with EinIndexers indexers at location ind.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.match_rule-Tuple{Any, Any}","page":"Manual","title":"OMEinsum.match_rule","text":"match_rule(ixs, iy)\nmatch_rule(code::EinCode)\n\nReturns the rule that matches, otherwise use DefaultRule - the slow loop_einsum backend.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.nopermute-Tuple{NTuple{N, T} where {N, T}, NTuple{N, T} where {N, T}}","page":"Manual","title":"OMEinsum.nopermute","text":"nopermute(ix,iy)\n\ncheck that all values in iy that are also in ix have the same relative order,\n\nexample\n\njulia> using OMEinsum: nopermute\n\njulia> nopermute((1,2,3),(1,2))\ntrue\n\njulia> nopermute((1,2,3),(2,1))\nfalse\n\ne.g. nopermute((1,2,3),(1,2)) is true while nopermute((1,2,3),(2,1)) is false\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.parse_parens-Tuple{AbstractString, Any, Any}","page":"Manual","title":"OMEinsum.parse_parens","text":"parse_parens(s::AbstractString, i, narg)\n\nparse one level of parens starting at index i where narg counts which tensor the current group of indices, e.g. \"ijk\", belongs to. Recursively calls itself for each new opening paren that's opened.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.tensorpermute!-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, AbstractArray{T, N}, Any, Any, Any}} where {T, N}","page":"Manual","title":"OMEinsum.tensorpermute!","text":"tensorpermute(A, perm)\n\npermutedims(A, perm) with grouped dimensions.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsum.@ein!-Tuple","page":"Manual","title":"OMEinsum.@ein!","text":"@ein! A[i,k] := B[i,j] * C[j,k]     # A = B * C\n@ein! A[i,k] += B[i,j] * C[j,k]     # A += B * C\n\nMacro interface similar to that of other packages.\n\nInplace version of @ein. \n\nexample\n\njulia> a, b, c, d = rand(2,2), rand(2,2), rand(2,2), zeros(2,2);\n\njulia> cc = copy(c);\n\njulia> @ein! d[i,k] := a[i,j] * b[j,k];\n\njulia> d ≈ a * b\ntrue\n\njulia> d ≈ ein\"ij,jk -> ik\"(a,b)\ntrue\n\njulia> @ein! c[i,k] += a[i,j] * b[j,k];\n\njulia> c ≈ cc + a * b\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#OMEinsum.@ein-Tuple","page":"Manual","title":"OMEinsum.@ein","text":"@ein A[i,k] := B[i,j] * C[j,k]     # A = B * C\n\nMacro interface similar to that of other packages.\n\nYou may use numbers in place of letters for dummy indices, as in @tensor, and need not name the output array. Thus A = @ein [1,2] := B[1,ξ] * C[ξ,2] is equivalent to the above. This can also be written A = ein\"ij,jk -> ik\"(B,C) using the numpy-style string macro.\n\nexample\n\njulia> a, b = rand(2,2), rand(2,2);\n\njulia> @ein c[i,k] := a[i,j] * b[j,k];\n\njulia> c ≈ a * b\ntrue\n\njulia> c ≈ ein\"ij,jk -> ik\"(a,b)\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#OMEinsum.@ein_str-Tuple{AbstractString}","page":"Manual","title":"OMEinsum.@ein_str","text":"ein\"ij,jk -> ik\"(A,B)\n\nString macro interface which understands numpy.einsum's notation. Translates strings into StaticEinCode-structs that can be called to evaluate an einsum. To control evaluation order, use parentheses - instead of an EinCode, a NestedEinsum is returned which evaluates the expression according to parens. The valid character ranges for index-labels are a-z and α-ω.\n\nexample\n\njulia> a, b, c = rand(10,10), rand(10,10), rand(10,1);\n\njulia> ein\"ij,jk,kl -> il\"(a,b,c) ≈ ein\"(ij,jk),kl -> il\"(a,b,c) ≈ a * b * c\ntrue\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#OMEinsum.@optein_str-Tuple{AbstractString}","page":"Manual","title":"OMEinsum.@optein_str","text":"optein\"ij,jk,kl -> ik\"(A, B, C)\n\nString macro interface that similar to @ein_str, with optimized contraction order (dimensions are assumed to be uniform).\n\n\n\n\n\n","category":"macro"},{"location":"docstrings/#OMEinsumContractionOrders.ExactTreewidth","page":"Manual","title":"OMEinsumContractionOrders.ExactTreewidth","text":"const ExactTreewidth{GM} = Treewidth{SafeRules{BT, MMW{3}(), MF}, GM}\nExactTreewidth(; greedy_config = GreedyMethod(nrepeat=1)) = Treewidth(; greedy_config)\n\nExactTreewidth is a specialization of Treewidth for the SafeRules preprocessing algorithm with the BT elimination algorithm. The BT algorithm is an exact solver for the treewidth problem that implemented in TreeWidthSolver.jl.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.GreedyMethod","page":"Manual","title":"OMEinsumContractionOrders.GreedyMethod","text":"GreedyMethod{MT}\nGreedyMethod(; α = 0.0, temperature = 0.0, nrepeat=1)\n\nThe fast but poor greedy optimizer. Input arguments are\n\n* `α` is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2))\n* `temperature` is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\n* `nrepeat` is the number of repeatition, returns the best contraction order.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.KaHyParBipartite","page":"Manual","title":"OMEinsumContractionOrders.KaHyParBipartite","text":"KaHyParBipartite{RT,IT,GM}\nKaHyParBipartite(; sc_target, imbalances=collect(0.0:0.005:0.8),\n    max_group_size=40, greedy_config=GreedyMethod())\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. This program first recursively cuts the tensors into several groups using KaHyPar, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nimbalances is a KaHyPar parameter that controls the group sizes in hierarchical bipartition,\nmax_group_size is the maximum size that allowed to used greedy search,\ngreedy_config is a greedy optimizer.\n\nReferences\n\nHyper-optimized tensor network contraction\nSimulating the Sycamore quantum supremacy circuits\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.MergeGreedy","page":"Manual","title":"OMEinsumContractionOrders.MergeGreedy","text":"MergeGreedy <: CodeSimplifier\nMergeGreedy(; threshhold=-1e-12)\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges tensors greedily if the space complexity of merged tensors is reduced (difference smaller than the threshhold).\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.MergeVectors","page":"Manual","title":"OMEinsumContractionOrders.MergeVectors","text":"MergeVectors <: CodeSimplifier\nMergeVectors()\n\nContraction code simplifier (in order to reduce the time of calling optimizers) that merges vectors to closest tensors.\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.SABipartite","page":"Manual","title":"OMEinsumContractionOrders.SABipartite","text":"SABipartite{RT,BT}\nSABipartite(; sc_target=25, ntrials=50, βs=0.1:0.2:15.0, niters=1000\n    max_group_size=40, greedy_config=GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. This program first recursively cuts the tensors into several groups using simulated annealing, with maximum group size specifed by max_group_size and maximum space complexity specified by sc_target, Then finds the contraction order inside each group with the greedy search algorithm. Other arguments are\n\nsize_dict, a dictionary that specifies leg dimensions,\nsc_target is the target space complexity, defined as log2(number of elements in the largest tensor),\nmax_group_size is the maximum size that allowed to used greedy search,\nβs is a list of inverse temperature 1/T,\nniters is the number of iteration in each temperature,\nntrials is the number of repetition (with different random seeds),\nsub_optimizer, the optimizer for the bipartited sub graphs, one can choose GreedyMethod() or TreeSA(),\ninitializer, the partition configuration initializer, one can choose :random or :greedy (slow but better).\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.TreeSA","page":"Manual","title":"OMEinsumContractionOrders.TreeSA","text":"TreeSA{RT,IT,GM}\nTreeSA(; sc_target=20, βs=collect(0.01:0.05:15), ntrials=10, niters=50,\n    sc_weight=1.0, rw_weight=0.2, initializer=:greedy, greedy_config=GreedyMethod(; nrepeat=1))\n\nOptimize the einsum contraction pattern using the simulated annealing on tensor expression tree.\n\nsc_target is the target space complexity,\nntrials, βs and niters are annealing parameters, doing ntrials indepedent annealings, each has inverse tempteratures specified by βs, in each temperature, do niters updates of the tree.\nsc_weight is the relative importance factor of space complexity in the loss compared with the time complexity.\nrw_weight is the relative importance factor of memory read and write in the loss compared with the time complexity.\ninitializer specifies how to determine the initial configuration, it can be :greedy or :random. If it is using :greedy method to generate the initial configuration, it also uses two extra arguments greedy_method and greedy_nrepeat.\nnslices is the number of sliced legs, default is 0.\nfixed_slices is a vector of sliced legs, default is [].\n\nReferences\n\nRecursive Multi-Tensor Contraction for XEB Verification of Quantum Circuits\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.Treewidth","page":"Manual","title":"OMEinsumContractionOrders.Treewidth","text":"struct Treewidth{EL <: EliminationAlgorithm, GM} <: CodeOptimizer\nTreewidth(; alg::EL = SafeRules(BT(), MMW{3}(), MF()), greedy_config::GM = GreedyMethod(nrepeat=1))\n\nTree width based solver. The solvers are implemented in CliqueTrees.jl and TreeWidthSolver.jl. They include:\n\nAlgorithm Description Time Complexity Space Complexity\nBFS breadth-first search O(m + n) O(n)\nMCS maximum cardinality search O(m + n) O(n)\nLexBFS lexicographic breadth-first search O(m + n) O(m + n)\nRCMMD reverse Cuthill-Mckee (minimum degree) O(m + n) O(m + n)\nRCMGL reverse Cuthill-Mckee (George-Liu) O(m + n) O(m + n)\nMCSM maximum cardinality search (minimal) O(mn) O(n)\nLexM lexicographic breadth-first search (minimal) O(mn) O(n)\nAMF approximate minimum fill O(mn) O(m + n)\nMF minimum fill O(mn²) -\nMMD multiple minimum degree O(mn²) O(m + n)\n\nDetailed descriptions is available in the CliqueTrees.jl.\n\nFields\n\nalg::EL: The algorithm to use for the treewidth calculation. Available elimination algorithms are listed above.\ngreedy_config::GM: The configuration for the greedy method.\n\nExample\n\njulia> optimizer = Treewidth();\n\njulia> eincode = OMEinsumContractionOrders.EinCode([['a', 'b'], ['a', 'c', 'd'], ['b', 'c', 'e', 'f'], ['e'], ['d', 'f']], ['a'])\nab, acd, bcef, e, df -> a\n\njulia> size_dict = Dict([c=>(1<<i) for (i,c) in enumerate(['a', 'b', 'c', 'd', 'e', 'f'])]...)\nDict{Char, Int64} with 6 entries:\n  'f' => 64\n  'a' => 2\n  'c' => 8\n  'd' => 16\n  'e' => 32\n  'b' => 4\n\njulia> optcode = optimize_code(eincode, size_dict, optimizer)\nab, ab -> a\n├─ fac, bcf -> ab\n│  ├─ df, acd -> fac\n│  │  ├─ df\n│  │  └─ acd\n│  └─ e, bcef -> bcf\n│     ├─ e\n│     └─ bcef\n└─ ab\n\n\n\n\n\n","category":"type"},{"location":"docstrings/#OMEinsumContractionOrders.contraction_complexity-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Manual","title":"OMEinsumContractionOrders.contraction_complexity","text":"contraction_complexity(eincode, size_dict) -> ContractionComplexity\n\nReturns the time, space and read-write complexity of the einsum contraction. The returned object contains 3 fields:\n\ntime complexity tc defined as log2(number of element-wise multiplications).\nspace complexity sc defined as log2(size of the maximum intermediate tensor).\nread-write complexity rwc defined as log2(the number of read-write operations).\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.flop-Union{Tuple{VT}, Tuple{LT}, Tuple{OMEinsumContractionOrders.EinCode, Dict{LT, VT}}} where {LT, VT}","page":"Manual","title":"OMEinsumContractionOrders.flop","text":"flop(eincode, size_dict) -> Int\n\nReturns the number of iterations, which is different with the true floating point operations (FLOP) by a factor of 2.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.label_elimination_order-Tuple{OMEinsumContractionOrders.NestedEinsum}","page":"Manual","title":"OMEinsumContractionOrders.label_elimination_order","text":"label_elimination_order(code) -> Vector\n\nReturns a vector of labels sorted by the order they are eliminated in the contraction tree. The contraction tree is specified by code, which e.g. can be a NestedEinsum instance.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_code","page":"Manual","title":"OMEinsumContractionOrders.optimize_code","text":"optimize_code(eincode, size_dict, optimizer = GreedyMethod(), simplifier=nothing, permute=true) -> optimized_eincode\n\nOptimize the einsum contraction code and reduce the time/space complexity of tensor network contraction. Returns a NestedEinsum instance. Input arguments are\n\neincode is an einsum contraction code instance, one of DynamicEinCode, StaticEinCode or NestedEinsum.\nsize is a dictionary of \"edge label=>edge size\" that contains the size information, one can use uniformsize(eincode, 2) to create a uniform size.\noptimizer is a CodeOptimizer instance, should be one of GreedyMethod, Treewidth, KaHyParBipartite, SABipartite or TreeSA. Check their docstrings for details.\nsimplifier is one of MergeVectors or MergeGreedy.\noptimize the permutation if permute is true.\n\nExamples\n\njulia> using OMEinsum\n\njulia> code = ein\"ij, jk, kl, il->\"\nij, jk, kl, il -> \n\njulia> optimize_code(code, uniformsize(code, 2), TreeSA());\n\n\n\n\n\n","category":"function"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_greedy-Union{Tuple{T2}, Tuple{TT}, Tuple{TA}, Tuple{L}, Tuple{OMEinsumContractionOrders.EinCode{L}, Dict{L, T2}}} where {L, TA, TT, T2}","page":"Manual","title":"OMEinsumContractionOrders.optimize_greedy","text":"optimize_greedy(eincode, size_dict; α = 0.0, temperature = 0.0, nrepeat=1)\n\nGreedy optimizing the contraction order and return a NestedEinsum object. Check the docstring of tree_greedy for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_kahypar-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Manual","title":"OMEinsumContractionOrders.optimize_kahypar","text":"optimize_kahypar(code, size_dict; sc_target, max_group_size=40, imbalances=0.0:0.01:0.2, greedy_method=MinSpaceOut(), greedy_nrepeat=1)\n\nOptimize the einsum code contraction order using the KaHyPar + Greedy approach. size_dict is a dictionary that specifies leg dimensions.  Check the docstring of KaHyParBipartite for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_kahypar_auto-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Manual","title":"OMEinsumContractionOrders.optimize_kahypar_auto","text":"optimize_kahypar_auto(code, size_dict; max_group_size=40, sub_optimizer = GreedyMethod())\n\nFind the optimal contraction order automatically by determining the sc_target with bisection. It can fail if the tree width of your graph is larger than 100.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_sa-Tuple{OMEinsumContractionOrders.EinCode, Any}","page":"Manual","title":"OMEinsumContractionOrders.optimize_sa","text":"optimize_sa(code, size_dict; sc_target, max_group_size=40, βs=0.1:0.2:15.0, niters=1000, ntrials=50,\n       sub_optimizer = GreedyMethod(), initializer=:random)\n\nOptimize the einsum code contraction order using the Simulated Annealing bipartition + Greedy approach. size_dict is a dictionary that specifies leg dimensions.  Check the docstring of SABipartite for detailed explaination of other input arguments.\n\nReferences\n\nHyper-optimized tensor network contraction\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_tree-Tuple{OMEinsumContractionOrders.AbstractEinsum, Any}","page":"Manual","title":"OMEinsumContractionOrders.optimize_tree","text":"optimize_tree(code, size_dict; sc_target=20, βs=0.1:0.1:10, ntrials=2, niters=100, sc_weight=1.0, rw_weight=0.2, initializer=:greedy, greedy_method=MinSpaceOut(), fixed_slices=[])\n\nOptimize the einsum contraction pattern specified by code, and edge sizes specified by size_dict. Check the docstring of TreeSA for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.optimize_treewidth-Union{Tuple{EL}, Tuple{GM}, Tuple{OMEinsumContractionOrders.Treewidth{EL, GM}, OMEinsumContractionOrders.AbstractEinsum, Dict}} where {GM, EL}","page":"Manual","title":"OMEinsumContractionOrders.optimize_treewidth","text":"optimize_treewidth(optimizer, eincode, size_dict)\n\nOptimizing the contraction order via solve the exact tree width of the line graph corresponding to the eincode and return a NestedEinsum object. Check the docstring of treewidth_method for detailed explaination of other input arguments.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.peak_memory-Tuple{OMEinsumContractionOrders.NestedEinsum, Dict}","page":"Manual","title":"OMEinsumContractionOrders.peak_memory","text":"peak_memory(code, size_dict::Dict) -> Int\n\nEstimate peak memory in number of elements.\n\n\n\n\n\n","category":"method"},{"location":"docstrings/#OMEinsumContractionOrders.tree_greedy-Union{Tuple{ET}, Tuple{TT}, Tuple{TA}, Tuple{OMEinsumContractionOrders.IncidenceList{Int64, ET}, Any}} where {TA, TT, ET}","page":"Manual","title":"OMEinsumContractionOrders.tree_greedy","text":"tree_greedy(incidence_list, log2_sizes; α = 0.0, temperature = 0.0, nrepeat=1)\n\nCompute greedy order, and the time and space complexities, the rows of the incidence_list are vertices and columns are edges. log2_sizes are defined on edges. α is the parameter for the loss function, for pairwise interaction, L = size(out) - α * (size(in1) + size(in2)) temperature is the parameter for sampling, if it is zero, the minimum loss is selected; for non-zero, the loss is selected by the Boltzmann distribution, given by p ~ exp(-loss/temperature).\n\n\n\n\n\n","category":"method"}]
}
